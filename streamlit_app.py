{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5827f1-5a67-4c0e-ac69-be3528f68c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-22 21:07:19.289 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.292 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-11-22 21:07:19.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "# ----------- CONFIG ----------------\n",
    "DB_FAISS_PATH = \"vectorstore\"   # <-- Correct folder only\n",
    "\n",
    "\n",
    "@st.cache_resource\n",
    "def get_vectorstore():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
    "    return db\n",
    "\n",
    "\n",
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt = PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def main():\n",
    "    st.title(\"ðŸ§  AI Medical RAG Chatbot\")\n",
    "\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        st.chat_message(message['role']).markdown(message['content'])\n",
    "\n",
    "    query = st.chat_input(\"Ask your medical question...\")\n",
    "\n",
    "    if query:\n",
    "\n",
    "        st.chat_message(\"user\").markdown(query)\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "        try:\n",
    "            vectorstore = get_vectorstore()\n",
    "\n",
    "            if vectorstore is None:\n",
    "                st.error(\"âŒ Failed to load vectorstore\")\n",
    "                return\n",
    "\n",
    "            # ----------- LLM -------------\n",
    "            GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "            llm = ChatGroq(\n",
    "                model=\"llama-3.1-8b-instant\",\n",
    "                temperature=0.2,\n",
    "                api_key=GROQ_API_KEY\n",
    "            )\n",
    "\n",
    "            # ----------- Prompt from Hub -------------\n",
    "            retrieval_prompt = hub.pull(\"langchain-ai/rag-prompt\")\n",
    "\n",
    "            # ----------- Create Chain -------------\n",
    "            combine_docs_chain = create_stuff_documents_chain(llm, retrieval_prompt)\n",
    "\n",
    "            rag_chain = create_retrieval_chain(\n",
    "                retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "                combine_documents_chain=combine_docs_chain,\n",
    "            )\n",
    "\n",
    "            response = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "            result = response[\"output_text\"]  # <-- New correct key\n",
    "\n",
    "            st.chat_message(\"assistant\").markdown(result)\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"âš  ERROR: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "id": "b69d1ff5-1e73-42ae-aa07-2b072a5d4685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Medibot Env",
   "language": "python",
   "name": "medibot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
